{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bd07612",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efac3328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.smallcnn import SmallCNN\n",
    "from src.preprocessed_dataset import PrecomputedFMADataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from src.audio_processing import audio_to_mel_tensor\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from pathlib import Path\n",
    "from src.rnn import AudioRNN\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8979cf9",
   "metadata": {},
   "source": [
    "Data préparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b7d61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remi-moustamsik/Documents/projet_lock/src/audio_processing.py:7: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(path, sr=sr, mono=True)\n",
      "[src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n",
      "/Users/remi-moustamsik/Documents/projet_lock/lock-env/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "Python(18148) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur de chargement fma_small/133/133297.mp3: \n",
      "On ignore le fichier corrompu : fma_small/133/133297.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/layer3.c:INT123_do_layer3():1776] error: part2_3_length (3360) too large for available bit count (3240)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1776] error: part2_3_length (3328) too large for available bit count (3240)\n",
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 33361.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "/Users/remi-moustamsik/Documents/projet_lock/src/audio_processing.py:7: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(path, sr=sr, mono=True)\n",
      "/Users/remi-moustamsik/Documents/projet_lock/lock-env/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 22401.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 63168.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "[src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n",
      "Python(18483) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur de chargement fma_small/099/099134.mp3: \n",
      "On ignore le fichier corrompu : fma_small/099/099134.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remi-moustamsik/Documents/projet_lock/src/audio_processing.py:7: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(path, sr=sr, mono=True)\n",
      "[src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n",
      "/Users/remi-moustamsik/Documents/projet_lock/lock-env/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "Python(19011) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur de chargement fma_small/108/108925.mp3: \n",
      "On ignore le fichier corrompu : fma_small/108/108925.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/layer3.c:INT123_do_layer3():1844] error: dequantization failed!\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de fichiers valides : 7997\n"
     ]
    }
   ],
   "source": [
    "root = \"fma_small\"\n",
    "file_paths = []\n",
    "labels = []\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(root):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".mp3\"):\n",
    "            full_path = os.path.join(dirpath, filename)\n",
    "\n",
    "            label = os.path.basename(dirpath)\n",
    "\n",
    "            file_paths.append(full_path)\n",
    "            labels.append(int(label))\n",
    "\n",
    "valid_rows = []\n",
    "\n",
    "tracks_df = pd.read_csv(\"fma_metadata/tracks.csv\", header=[0,1], index_col=0)\n",
    "\n",
    "genres_raw = tracks_df[(\"track\", \"genres\")]\n",
    "track_genre_id_map = {}\n",
    "\n",
    "for tid, string_list in genres_raw.items():\n",
    "    try:\n",
    "        genre_ids = ast.literal_eval(string_list)\n",
    "        if len(genre_ids) > 0:\n",
    "            track_genre_id_map[tid] = genre_ids[0]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "track_genre_ids = []\n",
    "\n",
    "for path in file_paths:\n",
    "    filename = os.path.basename(path)\n",
    "    track_id = int(filename.replace(\".mp3\", \"\"))\n",
    "\n",
    "    if track_id in track_genre_id_map:\n",
    "        track_genre_ids.append(track_genre_id_map[track_id])\n",
    "    else:\n",
    "        track_genre_ids.append(None)  \n",
    "\n",
    "for path, gid in zip(file_paths, track_genre_ids):\n",
    "    if gid is None:\n",
    "        continue\n",
    "\n",
    "    mel, sr = audio_to_mel_tensor(path)\n",
    "    if mel is None:\n",
    "        print(f\"On ignore le fichier corrompu : {path}\")\n",
    "        continue\n",
    "\n",
    "    valid_rows.append((path, gid))\n",
    "\n",
    "print(f\"Nombre de fichiers valides : {len(valid_rows)}\")\n",
    "\n",
    "# On reconstruit les listes propres\n",
    "valid_paths = [r[0] for r in valid_rows]\n",
    "valid_genre_ids = [r[1] for r in valid_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f45819",
   "metadata": {},
   "source": [
    "Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa5487b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de classes après filtrage: 103\n"
     ]
    }
   ],
   "source": [
    "unique_genre_ids = sorted(set(valid_genre_ids))\n",
    "genre_id_to_idx = {gid: i for i, gid in enumerate(unique_genre_ids)}\n",
    "idx_to_genre_id = {i: gid for gid, i in genre_id_to_idx.items()}\n",
    "num_classes = len(unique_genre_ids)\n",
    "print(\"Nombre de classes après filtrage:\", num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b34154",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca38501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some grad stats: 113.45136260986328\n",
      "num_classes var: 103\n",
      "label_idx max: 102 n_unique: 103\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "inputs, labels = next(iter(train_loader))\n",
    "inputs = inputs.to(device).float(); labels = labels.to(device)\n",
    "logits, _ = model(inputs)\n",
    "loss = criterion(logits, labels)\n",
    "loss.backward()\n",
    "print(\"some grad stats:\", model.conv_block1[0].weight.grad.norm().item())\n",
    "\n",
    "import pandas as pd\n",
    "md = pd.read_csv(\"precomputed_mels/metadata.csv\")\n",
    "print(\"num_classes var:\", num_classes)\n",
    "print(\"label_idx max:\", md[\"label_idx\"].max(), \"n_unique:\", md[\"label_idx\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9ca5a1",
   "metadata": {},
   "source": [
    "Stockage features précalculées.\n",
    "Cela permet de gagner du temps en évitant de process les fichierrs audio à chque époque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab624ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/layer3.c:INT123_do_layer3():1776] error: part2_3_length (3360) too large for available bit count (3240)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1776] error: part2_3_length (3328) too large for available bit count (3240)\n",
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 33361.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "/Users/remi-moustamsik/Documents/projet_lock/src/audio_processing.py:7: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(path, sr=sr, mono=True)\n",
      "/Users/remi-moustamsik/Documents/projet_lock/lock-env/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 22401.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 63168.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1844] error: dequantization failed!\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n"
     ]
    }
   ],
   "source": [
    "features_root = Path(\"precomputed_mels\")\n",
    "features_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "metadata_rows = []\n",
    "\n",
    "for path, gid in zip(valid_paths, valid_genre_ids):\n",
    "    # On calcule le mel\n",
    "    mel, sr = audio_to_mel_tensor(path)\n",
    "    if mel is None:\n",
    "        continue\n",
    "\n",
    "    mel = mel.float()\n",
    "\n",
    "    filename = os.path.basename(path)\n",
    "    track_id_str = filename.replace(\".mp3\", \"\")\n",
    "\n",
    "    out_path = features_root / f\"{track_id_str}.pt\"\n",
    "\n",
    "    # Sauvegarde tensor sur disque\n",
    "    torch.save(mel, out_path)\n",
    "\n",
    "    label_idx = genre_id_to_idx[gid]\n",
    "\n",
    "    metadata_rows.append({\n",
    "        \"track_id\": int(track_id_str),\n",
    "        \"audio_path\": path,\n",
    "        \"feature_path\": str(out_path),\n",
    "        \"genre_id\": gid,\n",
    "        \"label_idx\": label_idx,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3db392e",
   "metadata": {},
   "source": [
    "Sauvegarder les metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b93f628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pré-calcul terminé.\n",
      "Features sauvegardées dans : precomputed_mels\n",
      "Metadata : precomputed_mels/metadata.csv\n"
     ]
    }
   ],
   "source": [
    "metadata_df = pd.DataFrame(metadata_rows)\n",
    "metadata_df.to_csv(features_root / \"metadata.csv\", index=False)\n",
    "\n",
    "print(\"✅ Pré-calcul terminé.\")\n",
    "print(f\"Features sauvegardées dans : {features_root}\")\n",
    "print(f\"Metadata : {features_root / 'metadata.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5267db74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Entraînement sur GPU Apple (MPS)\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"✓ Entraînement sur GPU Apple (MPS)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"⚠️ Entraînement sur CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "81d034f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_csv = \"precomputed_mels/metadata.csv\"\n",
    "\n",
    "dataset = PrecomputedFMADataset(metadata_csv, max_len=3000)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size  = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "83c54dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device).float()\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits, _ = model(inputs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f1288d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device).float()\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            logits, probs = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            _, preds = torch.max(probs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return epoch_loss, accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "32014ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmallCNN(\n",
       "  (conv_block1): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (conv_block2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (conv_block3): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (conv_block4): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=128, out_features=103, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SmallCNN(n_mels=128, num_classes=num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d366972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape, dtype, device: torch.Size([2, 1, 128, 3000]) torch.float32\n",
      "labels sample, min/max: tensor([46, 46]) 46 46\n",
      "feature stats: -1.9115344285964966 -1.017252593271678e-08 3.153644561767578\n",
      "Index(['track_id', 'audio_path', 'feature_path', 'genre_id', 'label_idx'], dtype='object')\n",
      "unique genre_id: 103 unique label_idx: 103\n",
      "label_idx unique sample: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9)]\n",
      "Input shape: torch.Size([2, 1, 128, 3000]), dtype: torch.float32, device: mps:0\n",
      "Labels shape: torch.Size([2]), dtype: torch.int64, device: mps:0\n",
      "Logits shape: torch.Size([2, 103])\n",
      "Probs shape: torch.Size([2, 103])\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 20\n",
    "\n",
    "list_epoch = []\n",
    "list_test_loss = []\n",
    "list_test_acc = []\n",
    "list_train_loss = []\n",
    "\n",
    "metadata_csv = \"precomputed_mels/metadata.csv\"\n",
    "\n",
    "dataset = PrecomputedFMADataset(metadata_csv, max_len=3000)\n",
    "dataset_size = len(dataset)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size  = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=2, shuffle=False)\n",
    "\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    print(\"inputs shape, dtype, device:\", inputs.shape, inputs.dtype)\n",
    "    print(\"labels sample, min/max:\", labels[:10], labels.min().item(), labels.max().item())\n",
    "    print(\"feature stats:\", inputs.min().item(), inputs.mean().item(), inputs.max().item())\n",
    "    break\n",
    "\n",
    "import pandas as pd\n",
    "md = pd.read_csv(\"precomputed_mels/metadata.csv\")\n",
    "print(md.columns)\n",
    "print(\"unique genre_id:\", md[\"genre_id\"].nunique(), \"unique label_idx:\", md[\"label_idx\"].nunique())\n",
    "print(\"label_idx unique sample:\", sorted(md[\"label_idx\"].unique())[:10])\n",
    "\n",
    "for inputs, labels in train_loader:\n",
    "    inputs = inputs.to(device).float()\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    print(f\"Input shape: {inputs.shape}, dtype: {inputs.dtype}, device: {inputs.device}\")\n",
    "    print(f\"Labels shape: {labels.shape}, dtype: {labels.dtype}, device: {labels.device}\")\n",
    "    \n",
    "    logits, probs = model(inputs)\n",
    "    print(f\"Logits shape: {logits.shape}\")\n",
    "    print(f\"Probs shape: {probs.shape}\")\n",
    "    break\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # ---- TRAIN ----\n",
    "    train_loss = train_one_epoch(\n",
    "        model=model,\n",
    "        dataloader=train_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # ---- TEST ----\n",
    "    test_loss, test_acc = evaluate(\n",
    "        model=model,\n",
    "        dataloader=test_loader,\n",
    "        criterion=criterion,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    if torch.backends.mps.is_available():\n",
    "        try:\n",
    "            torch.mps.empty_cache()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    _, preds = torch.max(probs, dim=1)\n",
    "    print(f\"Sample preds: {preds[:5]}, Sample labels: {labels[:5]}\")\n",
    "    print(f\"Unique preds: {torch.unique(preds)}, Unique labels: {torch.unique(labels)}\")\n",
    "    list_epoch.append(epoch)\n",
    "    list_test_loss.append(test_loss)\n",
    "    list_test_acc.append(test_acc)\n",
    "    list_train_loss.append(train_loss)\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{NUM_EPOCHS} \"\n",
    "        f\"- train loss: {train_loss:.4f} | test loss: {test_loss:.4f} | test acc: {test_acc*100:.2f}%\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c59a34f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(list_epoch, train_loss, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(list_epoch, test_loss, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.plot(list_epoch, list_train_loss, label='Train Loss')\n",
    "plt.plot(list_epoch, list_test_loss, label='Test Loss')\n",
    "plt.plot(list_epoch, list_test_acc, label='Test Accuracy')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss / Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ebbbd07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96ad021",
   "metadata": {},
   "source": [
    "Code RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2aed135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AudioRNN(\n",
    "    n_mels=128,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    bidirectional=True,\n",
    "    embedding_dim=128,\n",
    "    num_classes=None,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fc8115e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "unique genre_id: 103 unique label_idx: 103\n",
      "label_idx unique sample: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AudioRNN(\n",
       "  (rnn): GRU(128, 128, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (embedding_proj): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (classifier): Linear(in_features=128, out_features=103, bias=True)\n",
       "  (_softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pandas as pd\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "metadata_csv = \"precomputed_mels/metadata.csv\"\n",
    "\n",
    "dataset = PrecomputedFMADataset(metadata_csv, max_len=3000)\n",
    "dataset_size = len(dataset)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size  = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=2, shuffle=False)\n",
    "\n",
    "# device MPS ou CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "md = pd.read_csv(metadata_csv)\n",
    "num_classes = md[\"label_idx\"].nunique()\n",
    "print(\"unique genre_id:\", md[\"genre_id\"].nunique(), \"unique label_idx:\", num_classes)\n",
    "print(\"label_idx unique sample:\", sorted(md[\"label_idx\"].unique())[:10])\n",
    "\n",
    "# ---- instancier le RNN ----\n",
    "model = AudioRNN(\n",
    "    n_mels=128,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    bidirectional=True,\n",
    "    embedding_dim=128,\n",
    "    num_classes=num_classes,\n",
    "    dropout=0.3,\n",
    ")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8f5091",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c51494ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 1, 128, 3000]), dtype: torch.float32, device: mps:0\n",
      "Labels shape: torch.Size([2]), dtype: torch.int64, device: mps:0\n",
      "Logits shape: torch.Size([2, 103])\n",
      "Probs shape: torch.Size([2, 103])\n",
      "Embeddings shape: torch.Size([2, 128])\n"
     ]
    }
   ],
   "source": [
    "for inputs, labels in train_loader:\n",
    "    inputs = inputs.to(device).float()\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    print(f\"Input shape: {inputs.shape}, dtype: {inputs.dtype}, device: {inputs.device}\")\n",
    "    print(f\"Labels shape: {labels.shape}, dtype: {labels.dtype}, device: {labels.device}\")\n",
    "\n",
    "    logits, probs, emb = model(inputs)\n",
    "    print(f\"Logits shape: {logits.shape}\")\n",
    "    print(f\"Probs shape: {probs.shape}\")\n",
    "    print(f\"Embeddings shape: {emb.shape}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2c51ed",
   "metadata": {},
   "source": [
    "Fonctions d'entrainement et d'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b43edded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device).float()\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, probs, emb = model(inputs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        running_correct += (preds == labels).sum().item()\n",
    "\n",
    "        del inputs, labels, logits, probs, emb, loss\n",
    "        if torch.backends.mps.is_available():\n",
    "            torch.mps.empty_cache()\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader.dataset)\n",
    "    acc = running_correct / len(dataloader.dataset)\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device).float()\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits, probs, emb = model(inputs)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        running_correct += (preds == labels).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader.dataset)\n",
    "    acc = running_correct / len(dataloader.dataset)\n",
    "    return avg_loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cf248ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward OK, elapsed: 2.5677108764648438\n"
     ]
    }
   ],
   "source": [
    "# ...existing code...\n",
    "# Test forward passe (si ça hang ici, c'est MPS+RNN)\n",
    "import time\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "x = torch.randn(1,1,128,3000, device=device, dtype=torch.float32)\n",
    "m = AudioRNN(n_mels=128, num_layers=2, bidirectional=True, num_classes=10).to(device)\n",
    "t0 = time.time()\n",
    "out = m(x)   # si blocage -> problème MPS+RNN\n",
    "print(\"forward OK, elapsed:\", time.time()-t0)\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8d32fa",
   "metadata": {},
   "source": [
    "Boucle d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "56306733",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# ---- TRAIN ----\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# ---- TEST ----\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m evaluate(\n\u001b[1;32m     13\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     14\u001b[0m         dataloader\u001b[38;5;241m=\u001b[39mtest_loader,\n\u001b[1;32m     15\u001b[0m         criterion\u001b[38;5;241m=\u001b[39mcriterion,\n\u001b[1;32m     16\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m     17\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[71], line 12\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 12\u001b[0m logits, probs, emb \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, labels)\n\u001b[1;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Documents/projet_lock/lock-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projet_lock/lock-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/projet_lock/src/rnn.py:74\u001b[0m, in \u001b[0;36mAudioRNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     71\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)           \u001b[38;5;66;03m# (B, T, 128) : T = temps, 128 = features\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# RNN\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m rnn_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m        \u001b[38;5;66;03m# rnn_out: (B, T, rnn_out_dim)\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Pooling temporel global : moyenne + max (plus riche qu'un seul last step)\u001b[39;00m\n\u001b[1;32m     77\u001b[0m mean_pool \u001b[38;5;241m=\u001b[39m rnn_out\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)       \u001b[38;5;66;03m# (B, rnn_out_dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/projet_lock/lock-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projet_lock/lock-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/projet_lock/lock-env/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1394\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m   1393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1394\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1395\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1397\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1398\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1399\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1400\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1401\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1406\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\n\u001b[1;32m   1407\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1408\u001b[0m         batch_sizes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1415\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m   1416\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    # ---- TRAIN ----\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model=model,\n",
    "        dataloader=train_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # ---- TEST ----\n",
    "    test_loss, test_acc = evaluate(\n",
    "        model=model,\n",
    "        dataloader=test_loader,\n",
    "        criterion=criterion,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    if torch.backends.mps.is_available():\n",
    "        try:\n",
    "            torch.mps.empty_cache()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{NUM_EPOCHS} \"\n",
    "        f\"- train loss: {train_loss:.4f} | train acc: {train_acc*100:.2f}% \"\n",
    "        f\"| test loss: {test_loss:.4f} | test acc: {test_acc*100:.2f}%\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lock-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
