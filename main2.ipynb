{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bd07612",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "efac3328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.smallcnn import SmallCNN\n",
    "from src.preprocessed_dataset import PrecomputedFMADataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from src.audio_processing import audio_to_mel_tensor\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from pathlib import Path\n",
    "from src.rnn import AudioRNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8979cf9",
   "metadata": {},
   "source": [
    "Data préparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b7d61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remi-moustamsik/Documents/projet_lock/src/audio_processing.py:7: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(path, sr=sr, mono=True)\n",
      "[src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n",
      "/Users/remi-moustamsik/Documents/projet_lock/lock-env/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "Python(18148) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur de chargement fma_small/133/133297.mp3: \n",
      "On ignore le fichier corrompu : fma_small/133/133297.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/layer3.c:INT123_do_layer3():1776] error: part2_3_length (3360) too large for available bit count (3240)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1776] error: part2_3_length (3328) too large for available bit count (3240)\n",
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 33361.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "/Users/remi-moustamsik/Documents/projet_lock/src/audio_processing.py:7: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(path, sr=sr, mono=True)\n",
      "/Users/remi-moustamsik/Documents/projet_lock/lock-env/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 22401.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 63168.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "[src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n",
      "Python(18483) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur de chargement fma_small/099/099134.mp3: \n",
      "On ignore le fichier corrompu : fma_small/099/099134.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remi-moustamsik/Documents/projet_lock/src/audio_processing.py:7: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(path, sr=sr, mono=True)\n",
      "[src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n",
      "/Users/remi-moustamsik/Documents/projet_lock/lock-env/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "Python(19011) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur de chargement fma_small/108/108925.mp3: \n",
      "On ignore le fichier corrompu : fma_small/108/108925.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/layer3.c:INT123_do_layer3():1844] error: dequantization failed!\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de fichiers valides : 7997\n"
     ]
    }
   ],
   "source": [
    "root = \"fma_small\"\n",
    "file_paths = []\n",
    "labels = []\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(root):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".mp3\"):\n",
    "            full_path = os.path.join(dirpath, filename)\n",
    "\n",
    "            label = os.path.basename(dirpath)\n",
    "\n",
    "            file_paths.append(full_path)\n",
    "            labels.append(int(label))\n",
    "\n",
    "valid_rows = []\n",
    "\n",
    "tracks_df = pd.read_csv(\"fma_metadata/tracks.csv\", header=[0,1], index_col=0)\n",
    "\n",
    "genres_raw = tracks_df[(\"track\", \"genres\")]\n",
    "track_genre_id_map = {}\n",
    "\n",
    "for tid, string_list in genres_raw.items():\n",
    "    try:\n",
    "        genre_ids = ast.literal_eval(string_list)\n",
    "        if len(genre_ids) > 0:\n",
    "            track_genre_id_map[tid] = genre_ids[0]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "track_genre_ids = []\n",
    "\n",
    "for path in file_paths:\n",
    "    filename = os.path.basename(path)\n",
    "    track_id = int(filename.replace(\".mp3\", \"\"))\n",
    "\n",
    "    if track_id in track_genre_id_map:\n",
    "        track_genre_ids.append(track_genre_id_map[track_id])\n",
    "    else:\n",
    "        track_genre_ids.append(None)  \n",
    "\n",
    "for path, gid in zip(file_paths, track_genre_ids):\n",
    "    if gid is None:\n",
    "        continue\n",
    "\n",
    "    mel, sr = audio_to_mel_tensor(path)\n",
    "    if mel is None:\n",
    "        print(f\"On ignore le fichier corrompu : {path}\")\n",
    "        continue\n",
    "\n",
    "    valid_rows.append((path, gid))\n",
    "\n",
    "print(f\"Nombre de fichiers valides : {len(valid_rows)}\")\n",
    "\n",
    "# On reconstruit les listes propres\n",
    "valid_paths = [r[0] for r in valid_rows]\n",
    "valid_genre_ids = [r[1] for r in valid_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f45819",
   "metadata": {},
   "source": [
    "Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa5487b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de classes après filtrage: 103\n"
     ]
    }
   ],
   "source": [
    "unique_genre_ids = sorted(set(valid_genre_ids))\n",
    "genre_id_to_idx = {gid: i for i, gid in enumerate(unique_genre_ids)}\n",
    "idx_to_genre_id = {i: gid for gid, i in genre_id_to_idx.items()}\n",
    "num_classes = len(unique_genre_ids)\n",
    "print(\"Nombre de classes après filtrage:\", num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b34154",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca38501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some grad stats: 113.45136260986328\n",
      "num_classes var: 103\n",
      "label_idx max: 102 n_unique: 103\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "inputs, labels = next(iter(train_loader))\n",
    "inputs = inputs.to(device).float(); labels = labels.to(device)\n",
    "logits, _ = model(inputs)\n",
    "loss = criterion(logits, labels)\n",
    "loss.backward()\n",
    "print(\"some grad stats:\", model.conv_block1[0].weight.grad.norm().item())\n",
    "\n",
    "import pandas as pd\n",
    "md = pd.read_csv(\"precomputed_mels/metadata.csv\")\n",
    "print(\"num_classes var:\", num_classes)\n",
    "print(\"label_idx max:\", md[\"label_idx\"].max(), \"n_unique:\", md[\"label_idx\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9ca5a1",
   "metadata": {},
   "source": [
    "Stockage features précalculées.\n",
    "Cela permet de gagner du temps en évitant de process les fichierrs audio à chque époque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab624ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/layer3.c:INT123_do_layer3():1776] error: part2_3_length (3360) too large for available bit count (3240)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1776] error: part2_3_length (3328) too large for available bit count (3240)\n",
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 33361.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "/Users/remi-moustamsik/Documents/projet_lock/src/audio_processing.py:7: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(path, sr=sr, mono=True)\n",
      "/Users/remi-moustamsik/Documents/projet_lock/lock-env/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 22401.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 63168.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1844] error: dequantization failed!\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n"
     ]
    }
   ],
   "source": [
    "features_root = Path(\"precomputed_mels\")\n",
    "features_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "metadata_rows = []\n",
    "\n",
    "for path, gid in zip(valid_paths, valid_genre_ids):\n",
    "    # On calcule le mel\n",
    "    mel, sr = audio_to_mel_tensor(path)\n",
    "    if mel is None:\n",
    "        continue\n",
    "\n",
    "    mel = mel.float()\n",
    "\n",
    "    filename = os.path.basename(path)\n",
    "    track_id_str = filename.replace(\".mp3\", \"\")\n",
    "\n",
    "    out_path = features_root / f\"{track_id_str}.pt\"\n",
    "\n",
    "    # Sauvegarde tensor sur disque\n",
    "    torch.save(mel, out_path)\n",
    "\n",
    "    label_idx = genre_id_to_idx[gid]\n",
    "\n",
    "    metadata_rows.append({\n",
    "        \"track_id\": int(track_id_str),\n",
    "        \"audio_path\": path,\n",
    "        \"feature_path\": str(out_path),\n",
    "        \"genre_id\": gid,\n",
    "        \"label_idx\": label_idx,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3db392e",
   "metadata": {},
   "source": [
    "Sauvegarder les metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b93f628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pré-calcul terminé.\n",
      "Features sauvegardées dans : precomputed_mels\n",
      "Metadata : precomputed_mels/metadata.csv\n"
     ]
    }
   ],
   "source": [
    "metadata_df = pd.DataFrame(metadata_rows)\n",
    "metadata_df.to_csv(features_root / \"metadata.csv\", index=False)\n",
    "\n",
    "print(\"✅ Pré-calcul terminé.\")\n",
    "print(f\"Features sauvegardées dans : {features_root}\")\n",
    "print(f\"Metadata : {features_root / 'metadata.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5267db74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Entraînement sur GPU Apple (MPS)\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"✓ Entraînement sur GPU Apple (MPS)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"⚠️ Entraînement sur CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d034f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_csv = \"precomputed_mels/metadata.csv\"\n",
    "\n",
    "dataset = PrecomputedFMADataset(metadata_csv, max_len=3000)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size  = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c54dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device).float()\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits, _ = model(inputs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1288d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device).float()\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            logits, probs = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            _, preds = torch.max(probs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return epoch_loss, accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "32014ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmallCNN(\n",
       "  (conv_block1): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (conv_block2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (conv_block3): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (conv_block4): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=128, out_features=103, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SmallCNN(n_mels=128, num_classes=num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7d366972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape, dtype, device: torch.Size([2, 1, 128, 3000]) torch.float32\n",
      "labels sample, min/max: tensor([ 6, 81]) 6 81\n",
      "feature stats: -1.9761807918548584 1.017252593271678e-08 2.61867356300354\n",
      "Index(['track_id', 'audio_path', 'feature_path', 'genre_id', 'label_idx'], dtype='object')\n",
      "unique genre_id: 103 unique label_idx: 103\n",
      "label_idx unique sample: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9)]\n",
      "Input shape: torch.Size([2, 1, 128, 3000]), dtype: torch.float32, device: mps:0\n",
      "Labels shape: torch.Size([2]), dtype: torch.int64, device: mps:0\n",
      "Logits shape: torch.Size([2, 103])\n",
      "Probs shape: torch.Size([2, 103])\n",
      "Sample preds: tensor([7, 8], device='mps:0'), Sample labels: tensor([6, 8], device='mps:0')\n",
      "Unique preds: tensor([7, 8], device='mps:0'), Unique labels: tensor([6, 8], device='mps:0')\n",
      "Epoch 1/20 - train loss: 3.2040 | test loss: 3.0253 | test acc: 25.44%\n",
      "Sample preds: tensor([7, 8], device='mps:0'), Sample labels: tensor([6, 8], device='mps:0')\n",
      "Unique preds: tensor([7, 8], device='mps:0'), Unique labels: tensor([6, 8], device='mps:0')\n",
      "Epoch 2/20 - train loss: 2.9786 | test loss: 2.9401 | test acc: 30.06%\n",
      "Sample preds: tensor([7, 8], device='mps:0'), Sample labels: tensor([6, 8], device='mps:0')\n",
      "Unique preds: tensor([7, 8], device='mps:0'), Unique labels: tensor([6, 8], device='mps:0')\n",
      "Epoch 3/20 - train loss: 2.8180 | test loss: 2.8196 | test acc: 30.19%\n",
      "Sample preds: tensor([7, 8], device='mps:0'), Sample labels: tensor([6, 8], device='mps:0')\n",
      "Unique preds: tensor([7, 8], device='mps:0'), Unique labels: tensor([6, 8], device='mps:0')\n",
      "Epoch 4/20 - train loss: 2.7235 | test loss: 2.7133 | test acc: 33.44%\n",
      "Sample preds: tensor([7, 8], device='mps:0'), Sample labels: tensor([6, 8], device='mps:0')\n",
      "Unique preds: tensor([7, 8], device='mps:0'), Unique labels: tensor([6, 8], device='mps:0')\n",
      "Epoch 5/20 - train loss: 2.6475 | test loss: 2.7253 | test acc: 33.25%\n",
      "Sample preds: tensor([7, 8], device='mps:0'), Sample labels: tensor([6, 8], device='mps:0')\n",
      "Unique preds: tensor([7, 8], device='mps:0'), Unique labels: tensor([6, 8], device='mps:0')\n",
      "Epoch 6/20 - train loss: 2.5797 | test loss: 2.6720 | test acc: 35.31%\n",
      "Sample preds: tensor([7, 8], device='mps:0'), Sample labels: tensor([6, 8], device='mps:0')\n",
      "Unique preds: tensor([7, 8], device='mps:0'), Unique labels: tensor([6, 8], device='mps:0')\n",
      "Epoch 7/20 - train loss: 2.5201 | test loss: 2.6178 | test acc: 36.81%\n",
      "Sample preds: tensor([7, 8], device='mps:0'), Sample labels: tensor([6, 8], device='mps:0')\n",
      "Unique preds: tensor([7, 8], device='mps:0'), Unique labels: tensor([6, 8], device='mps:0')\n",
      "Epoch 8/20 - train loss: 2.4638 | test loss: 2.6021 | test acc: 37.69%\n",
      "Sample preds: tensor([7, 8], device='mps:0'), Sample labels: tensor([6, 8], device='mps:0')\n",
      "Unique preds: tensor([7, 8], device='mps:0'), Unique labels: tensor([6, 8], device='mps:0')\n",
      "Epoch 9/20 - train loss: 2.4229 | test loss: 2.5743 | test acc: 37.38%\n",
      "Sample preds: tensor([7, 8], device='mps:0'), Sample labels: tensor([6, 8], device='mps:0')\n",
      "Unique preds: tensor([7, 8], device='mps:0'), Unique labels: tensor([6, 8], device='mps:0')\n",
      "Epoch 10/20 - train loss: 2.3774 | test loss: 2.5837 | test acc: 36.88%\n",
      "Sample preds: tensor([7, 8], device='mps:0'), Sample labels: tensor([6, 8], device='mps:0')\n",
      "Unique preds: tensor([7, 8], device='mps:0'), Unique labels: tensor([6, 8], device='mps:0')\n",
      "Epoch 11/20 - train loss: 2.3298 | test loss: 2.5579 | test acc: 38.31%\n",
      "Sample preds: tensor([7, 8], device='mps:0'), Sample labels: tensor([6, 8], device='mps:0')\n",
      "Unique preds: tensor([7, 8], device='mps:0'), Unique labels: tensor([6, 8], device='mps:0')\n",
      "Epoch 12/20 - train loss: 2.2863 | test loss: 2.5455 | test acc: 38.56%\n",
      "Sample preds: tensor([7, 8], device='mps:0'), Sample labels: tensor([6, 8], device='mps:0')\n",
      "Unique preds: tensor([7, 8], device='mps:0'), Unique labels: tensor([6, 8], device='mps:0')\n",
      "Epoch 13/20 - train loss: 2.2528 | test loss: 2.5382 | test acc: 38.12%\n",
      "Sample preds: tensor([7, 8], device='mps:0'), Sample labels: tensor([6, 8], device='mps:0')\n",
      "Unique preds: tensor([7, 8], device='mps:0'), Unique labels: tensor([6, 8], device='mps:0')\n",
      "Epoch 14/20 - train loss: 2.2185 | test loss: 2.4974 | test acc: 39.31%\n",
      "Sample preds: tensor([7, 8], device='mps:0'), Sample labels: tensor([6, 8], device='mps:0')\n",
      "Unique preds: tensor([7, 8], device='mps:0'), Unique labels: tensor([6, 8], device='mps:0')\n",
      "Epoch 15/20 - train loss: 2.1835 | test loss: 2.5747 | test acc: 38.25%\n",
      "Sample preds: tensor([7, 8], device='mps:0'), Sample labels: tensor([6, 8], device='mps:0')\n",
      "Unique preds: tensor([7, 8], device='mps:0'), Unique labels: tensor([6, 8], device='mps:0')\n",
      "Epoch 16/20 - train loss: 2.1521 | test loss: 2.5260 | test acc: 40.38%\n",
      "Sample preds: tensor([7, 8], device='mps:0'), Sample labels: tensor([6, 8], device='mps:0')\n",
      "Unique preds: tensor([7, 8], device='mps:0'), Unique labels: tensor([6, 8], device='mps:0')\n",
      "Epoch 17/20 - train loss: 2.1225 | test loss: 2.5161 | test acc: 40.00%\n",
      "Sample preds: tensor([7, 8], device='mps:0'), Sample labels: tensor([6, 8], device='mps:0')\n",
      "Unique preds: tensor([7, 8], device='mps:0'), Unique labels: tensor([6, 8], device='mps:0')\n",
      "Epoch 18/20 - train loss: 2.0908 | test loss: 2.5868 | test acc: 37.69%\n",
      "Sample preds: tensor([7, 8], device='mps:0'), Sample labels: tensor([6, 8], device='mps:0')\n",
      "Unique preds: tensor([7, 8], device='mps:0'), Unique labels: tensor([6, 8], device='mps:0')\n",
      "Epoch 19/20 - train loss: 2.0623 | test loss: 2.6129 | test acc: 38.19%\n",
      "Sample preds: tensor([7, 8], device='mps:0'), Sample labels: tensor([6, 8], device='mps:0')\n",
      "Unique preds: tensor([7, 8], device='mps:0'), Unique labels: tensor([6, 8], device='mps:0')\n",
      "Epoch 20/20 - train loss: 2.0334 | test loss: 2.6105 | test acc: 37.31%\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 20\n",
    "\n",
    "\n",
    "metadata_csv = \"precomputed_mels/metadata.csv\"\n",
    "\n",
    "dataset = PrecomputedFMADataset(metadata_csv, max_len=3000)\n",
    "dataset_size = len(dataset)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size  = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=2, shuffle=False)\n",
    "\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    print(\"inputs shape, dtype, device:\", inputs.shape, inputs.dtype)\n",
    "    print(\"labels sample, min/max:\", labels[:10], labels.min().item(), labels.max().item())\n",
    "    print(\"feature stats:\", inputs.min().item(), inputs.mean().item(), inputs.max().item())\n",
    "    break\n",
    "\n",
    "import pandas as pd\n",
    "md = pd.read_csv(\"precomputed_mels/metadata.csv\")\n",
    "print(md.columns)\n",
    "print(\"unique genre_id:\", md[\"genre_id\"].nunique(), \"unique label_idx:\", md[\"label_idx\"].nunique())\n",
    "print(\"label_idx unique sample:\", sorted(md[\"label_idx\"].unique())[:10])\n",
    "\n",
    "for inputs, labels in train_loader:\n",
    "    inputs = inputs.to(device).float()\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    print(f\"Input shape: {inputs.shape}, dtype: {inputs.dtype}, device: {inputs.device}\")\n",
    "    print(f\"Labels shape: {labels.shape}, dtype: {labels.dtype}, device: {labels.device}\")\n",
    "    \n",
    "    logits, probs = model(inputs)\n",
    "    print(f\"Logits shape: {logits.shape}\")\n",
    "    print(f\"Probs shape: {probs.shape}\")\n",
    "    break\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # ---- TRAIN ----\n",
    "    train_loss = train_one_epoch(\n",
    "        model=model,\n",
    "        dataloader=train_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # ---- TEST ----\n",
    "    test_loss, test_acc = evaluate(\n",
    "        model=model,\n",
    "        dataloader=test_loader,\n",
    "        criterion=criterion,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    if torch.backends.mps.is_available():\n",
    "        try:\n",
    "            torch.mps.empty_cache()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    _, preds = torch.max(probs, dim=1)\n",
    "    print(f\"Sample preds: {preds[:5]}, Sample labels: {labels[:5]}\")\n",
    "    print(f\"Unique preds: {torch.unique(preds)}, Unique labels: {torch.unique(labels)}\")\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{NUM_EPOCHS} \"\n",
    "        f\"- train loss: {train_loss:.4f} | test loss: {test_loss:.4f} | test acc: {test_acc*100:.2f}%\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ebbbd07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96ad021",
   "metadata": {},
   "source": [
    "Code RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2aed135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AudioRNN(\n",
    "    n_mels=128,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    bidirectional=True,\n",
    "    embedding_dim=128,\n",
    "    num_classes=None,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fc8115e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "unique genre_id: 103 unique label_idx: 103\n",
      "label_idx unique sample: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pandas as pd\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "metadata_csv = \"precomputed_mels/metadata.csv\"\n",
    "\n",
    "dataset = PrecomputedFMADataset(metadata_csv, max_len=3000)\n",
    "dataset_size = len(dataset)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size  = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=2, shuffle=False)\n",
    "\n",
    "# device MPS ou CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "md = pd.read_csv(metadata_csv)\n",
    "num_classes = md[\"label_idx\"].nunique()\n",
    "print(\"unique genre_id:\", md[\"genre_id\"].nunique(), \"unique label_idx:\", num_classes)\n",
    "print(\"label_idx unique sample:\", sorted(md[\"label_idx\"].unique())[:10])\n",
    "\n",
    "# ---- instancier le RNN ----\n",
    "model = AudioRNN(\n",
    "    n_mels=128,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    bidirectional=True,\n",
    "    embedding_dim=128,\n",
    "    num_classes=num_classes,\n",
    "    dropout=0.3,\n",
    ").to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8f5091",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c51494ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 1, 128, 3000]), dtype: torch.float32, device: mps:0\n",
      "Labels shape: torch.Size([2]), dtype: torch.int64, device: mps:0\n",
      "Logits shape: torch.Size([2, 103])\n",
      "Probs shape: torch.Size([2, 103])\n",
      "Embeddings shape: torch.Size([2, 128])\n"
     ]
    }
   ],
   "source": [
    "for inputs, labels in train_loader:\n",
    "    inputs = inputs.to(device).float()\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    print(f\"Input shape: {inputs.shape}, dtype: {inputs.dtype}, device: {inputs.device}\")\n",
    "    print(f\"Labels shape: {labels.shape}, dtype: {labels.dtype}, device: {labels.device}\")\n",
    "\n",
    "    logits, probs, emb = model(inputs)\n",
    "    print(f\"Logits shape: {logits.shape}\")\n",
    "    print(f\"Probs shape: {probs.shape}\")\n",
    "    print(f\"Embeddings shape: {emb.shape}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2c51ed",
   "metadata": {},
   "source": [
    "Fonctions d'entrainement et d'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b43edded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device).float()\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, probs, emb = model(inputs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        running_correct += (preds == labels).sum().item()\n",
    "\n",
    "        del inputs, labels, logits, probs, emb, loss\n",
    "        if torch.backends.mps.is_available():\n",
    "            torch.mps.empty_cache()\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader.dataset)\n",
    "    acc = running_correct / len(dataloader.dataset)\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device).float()\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits, probs, emb = model(inputs)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        running_correct += (preds == labels).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader.dataset)\n",
    "    acc = running_correct / len(dataloader.dataset)\n",
    "    return avg_loss, acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8d32fa",
   "metadata": {},
   "source": [
    "Boucle d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56306733",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    # ---- TRAIN ----\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model=model,\n",
    "        dataloader=train_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # ---- TEST ----\n",
    "    test_loss, test_acc = evaluate(\n",
    "        model=model,\n",
    "        dataloader=test_loader,\n",
    "        criterion=criterion,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    if torch.backends.mps.is_available():\n",
    "        try:\n",
    "            torch.mps.empty_cache()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{NUM_EPOCHS} \"\n",
    "        f\"- train loss: {train_loss:.4f} | train acc: {train_acc*100:.2f}% \"\n",
    "        f\"| test loss: {test_loss:.4f} | test acc: {test_acc*100:.2f}%\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lock-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
